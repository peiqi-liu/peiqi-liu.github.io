---
date: 2024-11-07 20:00:00
layout: research 
tags: real robots, real home, imitation learning, home robotics, learning from demonstration, robotics dataset,  robot foundational model, dynamic environments, zero-shot deployment
pills: generalization-home-robotics, semantic-scene-representation, large-pretrained-models
description: "DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation"
title: "DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation"
authors: "Peiqi Liu, Zhanqiu Guo, Mohit Warke, Soumith Chintala, Chris Paxton, Nur Muhammad Mahi Shafiullah*, Lerrel Pinto*"
paper_url: https://arxiv.org/abs/2411.04999
code_url: https://github.com/peiqi-liu/stretch_ai/tree/experiment/src/stretch/dynav
project_site_url: https://dynamem.github.io/
show_blog_link: false
show_card: true
highlight: "Best Paper Award"
venue: "Lifelong Learning for Home Robots (LLHomeRobot) Workshop, CoRL 2024"
local_video: assets/images/research/dynamem.mp4
---

Significant progress has been made in open-vocabulary mobile manipulation, where the goal is for a robot to perform tasks in any environment given a natural language description. However, most current systems assume a static environment, which limits the system's applicability in real-world scenarios where environments frequently change due to human intervention or the robot's own actions. In this work, we present DynaMem, a new approach to open-world mobile manipulation that uses a dynamic spatio-semantic memory to represent a robot's environment. DynaMem constructs a 3D data structure to maintain a dynamic memory of point clouds, and answers open-vocabulary object localization queries using multimodal LLMs or open-vocabulary features generated by state-of-the-art vision-language models. Powered by DynaMem, our robots can explore novel environments, search for objects not found in memory, and continuously update the memory as objects move, appear, or disappear in the scene. We run extensive experiments on the Stretch SE3 robots in three real and nine offline scenes, and achieve an average pick-and-drop success rate of 70% on non-stationary objects, which is more than a 2x improvement over state-of-the-art static systems.